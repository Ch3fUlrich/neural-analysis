{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88378a90",
   "metadata": {},
   "source": [
    "# HDF5 I/O with h5io\n",
    "\n",
    "This notebook demonstrates how to save and load arrays and DataFrames using `neural_analysis.utils.h5io` and how to filter pairs when loading via `load_hdf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e903b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:53:47.703414Z",
     "iopub.status.busy": "2025-11-06T10:53:47.703207Z",
     "iopub.status.idle": "2025-11-06T10:53:49.380223Z",
     "shell.execute_reply": "2025-11-06T10:53:49.379535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using temp dir: /tmp/tmp2q3ekzy9\n"
     ]
    }
   ],
   "source": [
    "# Imports and helpers\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neural_analysis.utils import h5io\n",
    "from neural_analysis.utils.io import load_hdf5\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "base = Path(tmpdir.name)\n",
    "print(\"Using temp dir:\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62516a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:53:49.381272Z",
     "iopub.status.busy": "2025-11-06T10:53:49.381120Z",
     "iopub.status.idle": "2025-11-06T10:53:49.559368Z",
     "shell.execute_reply": "2025-11-06T10:53:49.558597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /tmp/tmp2q3ekzy9/array_demo.h5\n",
      "Array roundtrip OK: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Array roundtrip with labels and attrs\n",
    "path = base / 'array_demo.h5'\n",
    "data = np.random.randn(100, 10).astype(np.float32)\n",
    "labels = np.array([f'sample_{i}' for i in range(data.shape[0])])\n",
    "attrs = {\n",
    "    'description': 'random normal features',\n",
    "    'version': 1,\n",
    "    'metadata': {'source': 'synthetic', 'dims': list(data.shape)}\n",
    "}\n",
    "\n",
    "# Save\n",
    "h5io(path, task='save', data=data, labels=labels, attrs=attrs)\n",
    "print('Saved to', path)\n",
    "\n",
    "# Load\n",
    "loaded_data, loaded_labels = h5io(path, task='load')\n",
    "\n",
    "# Validate roundtrip\n",
    "assert isinstance(loaded_data, np.ndarray)\n",
    "np.testing.assert_allclose(loaded_data, data)\n",
    "assert list(loaded_labels) == list(labels)\n",
    "print('Array roundtrip OK:', loaded_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5547c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:53:49.560473Z",
     "iopub.status.busy": "2025-11-06T10:53:49.560317Z",
     "iopub.status.idle": "2025-11-06T10:53:49.578741Z",
     "shell.execute_reply": "2025-11-06T10:53:49.578123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame roundtrip OK: (5, 3)\n"
     ]
    }
   ],
   "source": [
    "# Example 2: DataFrame roundtrip\n",
    "path_df = base / 'df_demo.h5'\n",
    "df = pd.DataFrame({\n",
    "    'neuron_id': [f'n{i}' for i in range(5)],\n",
    "    'firing_rate': np.random.rand(5),\n",
    "    'condition': ['A', 'B', 'A', 'B', 'A'],\n",
    "})\n",
    "labels_df = ['trial_1', 'trial_2', 'trial_3', 'trial_4', 'trial_5']\n",
    "\n",
    "h5io(path_df, task='save', data=df, labels=labels_df)\n",
    "loaded_df, loaded_labels_df = h5io(path_df, task='load')\n",
    "\n",
    "# Validate\n",
    "assert isinstance(loaded_df, pd.DataFrame)\n",
    "pd.testing.assert_frame_equal(loaded_df.reset_index(drop=True), df.reset_index(drop=True))\n",
    "assert list(loaded_labels_df) == labels_df\n",
    "print('DataFrame roundtrip OK:', loaded_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3bdf16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:53:49.579687Z",
     "iopub.status.busy": "2025-11-06T10:53:49.579592Z",
     "iopub.status.idle": "2025-11-06T10:53:49.657782Z",
     "shell.execute_reply": "2025-11-06T10:53:49.657078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_i</th>\n",
       "      <th>item_j</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item_i item_j  score\n",
       "0      A      C    0.8\n",
       "1      B      C    0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter pairs OK\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Filtering pairs on load (via load_hdf5)\n",
    "path_pairs = base / 'pairs_demo.h5'\n",
    "pairs_df = pd.DataFrame({\n",
    "    'item_i': ['A', 'A', 'B', 'C'],\n",
    "    'item_j': ['B', 'C', 'C', 'D'],\n",
    "    'score': [0.1, 0.8, 0.5, 0.9],\n",
    "})\n",
    "h5io(path_pairs, task='save', data=pairs_df, labels=None)\n",
    "\n",
    "wanted = [('A','C'), ('B','C')]\n",
    "(loaded_filtered, _), attrs = load_hdf5(path_pairs, filter_pairs=wanted, return_attrs=True)\n",
    "print('Filtered rows:')\n",
    "display(loaded_filtered)\n",
    "\n",
    "# Validate only desired pairs\n",
    "assert set(zip(loaded_filtered['item_i'], loaded_filtered['item_j'])) == set(wanted)\n",
    "print('Filter pairs OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b9fff",
   "metadata": {},
   "source": [
    "## Advanced HDF5 Operations\n",
    "\n",
    "The following examples demonstrate the hierarchical HDF5 structure and advanced functions for saving/loading result datasets with mixed scalar and array data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b6a2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved hierarchical results to hierarchical_results.h5\n",
      "  Structure: dataset_name / result_key / {scalar_data, array_data}\n"
     ]
    }
   ],
   "source": [
    "# Example 4: save_result_to_hdf5_dataset with mixed data types\n",
    "from neural_analysis.utils.io import (\n",
    "    save_result_to_hdf5_dataset,\n",
    "    load_results_from_hdf5_dataset,\n",
    "    get_hdf5_dataset_names,\n",
    "    get_hdf5_result_summary\n",
    ")\n",
    "\n",
    "# Create a hierarchical HDF5 file with multiple result datasets\n",
    "hdf5_path = base / 'hierarchical_results.h5'\n",
    "\n",
    "# Save multiple results under different comparison groups\n",
    "for session in ['session_001', 'session_002']:\n",
    "    for condition_pair in [('condA', 'condB'), ('condA', 'condC'), ('condB', 'condC')]:\n",
    "        # Generate sample results\n",
    "        result_key = f\"{condition_pair[0]}_vs_{condition_pair[1]}_wasserstein\"\n",
    "        value = np.random.rand() * 100  # Scalar metric\n",
    "        \n",
    "        # Generate pairwise comparison data (could be large arrays)\n",
    "        n_pairs = 50\n",
    "        pair_indices = [(i, j) for i in range(10) for j in range(10) if i < j][:n_pairs]\n",
    "        pair_values = np.random.rand(n_pairs)\n",
    "        \n",
    "        # Create structured array that HDF5 can handle\n",
    "        # Store as separate arrays: indices and values\n",
    "        pairs_indices = np.array(pair_indices, dtype=np.int32)\n",
    "        pairs_vals = np.array(pair_values, dtype=np.float64)\n",
    "        \n",
    "        # Save with scalar attributes and array datasets\n",
    "        save_result_to_hdf5_dataset(\n",
    "            save_path=hdf5_path,\n",
    "            dataset_name=session,\n",
    "            result_key=result_key,\n",
    "            scalar_data={\n",
    "                'dataset_i': condition_pair[0],\n",
    "                'dataset_j': condition_pair[1],\n",
    "                'metric': 'wasserstein',\n",
    "                'value': float(value),\n",
    "                'n_samples_i': 100,\n",
    "                'n_samples_j': 100,\n",
    "                'timestamp': '2024-01-15'\n",
    "            },\n",
    "            array_data={\n",
    "                'pairs_indices': pairs_indices,\n",
    "                'pairs_values': pairs_vals\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"✓ Saved hierarchical results to {hdf5_path.name}\")\n",
    "print(f\"  Structure: dataset_name / result_key / {{scalar_data, array_data}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebc2a8",
   "metadata": {},
   "source": [
    "### Viewing Dataset Names\n",
    "\n",
    "Use `get_hdf5_dataset_names()` to list all comparison groups and result keys in the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64cd9637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset names in HDF5 file:\n",
      "  session_001\n",
      "  session_002\n"
     ]
    }
   ],
   "source": [
    "# List all datasets in the hierarchical HDF5 file\n",
    "dataset_names = get_hdf5_dataset_names(hdf5_path)\n",
    "\n",
    "print(\"Dataset names in HDF5 file:\")\n",
    "for name in dataset_names:\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61200f5c",
   "metadata": {},
   "source": [
    "### Loading with Filtering\n",
    "\n",
    "Use `load_results_from_hdf5_dataset()` to load results with optional filtering by comparison name, dataset names, or metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d30575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 results from session_001\n",
      "\n",
      "Sample result keys:\n",
      "  condA_vs_condB_wasserstein\n",
      "  condA_vs_condC_wasserstein\n",
      "  condB_vs_condC_wasserstein\n"
     ]
    }
   ],
   "source": [
    "# Example 6a: Load all results from session_001\n",
    "results_session1 = load_results_from_hdf5_dataset(\n",
    "    save_path=hdf5_path,\n",
    "    dataset_name='session_001'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(results_session1['session_001'])} results from session_001\")\n",
    "print(\"\\nSample result keys:\")\n",
    "for key in list(results_session1['session_001'].keys())[:3]:\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04920e",
   "metadata": {},
   "source": [
    "### Summary DataFrame\n",
    "\n",
    "Use `get_hdf5_result_summary()` to generate a pandas DataFrame with all results and their metadata for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c6b14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary DataFrame:\n",
      "  dataset_i dataset_j       metric      value  n_samples_i  n_samples_j\n",
      "0     condA     condB  wasserstein  10.734410          100          100\n",
      "1     condA     condC  wasserstein  59.203765          100          100\n",
      "2     condB     condC  wasserstein  22.089291          100          100\n"
     ]
    }
   ],
   "source": [
    "# Example 7: Generate summary DataFrame\n",
    "summary_df = get_hdf5_result_summary(\n",
    "    save_path=hdf5_path,\n",
    "    dataset_name='session_001'\n",
    ")\n",
    "\n",
    "print(\"Summary DataFrame:\")\n",
    "print(summary_df[['dataset_i', 'dataset_j', 'metric', 'value', 'n_samples_i', 'n_samples_j']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401a32b",
   "metadata": {},
   "source": [
    "### Accessing Array Data\n",
    "\n",
    "Results can contain both scalar attributes and large array datasets (like pairwise comparison matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7526bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: condA_vs_condB_wasserstein\n",
      "  Attributes: {'dataset_i': 'condA', 'dataset_j': 'condB', 'metric': 'wasserstein', 'n_samples_i': np.int64(100), 'n_samples_j': np.int64(100), 'timestamp': '2024-01-15', 'value': np.float64(10.734410362419588)}\n",
      "  Arrays available: ['pairs_indices', 'pairs_values']\n",
      "  Pairs shape: (45, 2), values: (50,)\n"
     ]
    }
   ],
   "source": [
    "# Example 6b: Access specific result details\n",
    "result_key = list(results_session1['session_001'].keys())[0]\n",
    "result = results_session1['session_001'][result_key]\n",
    "\n",
    "print(f\"Result: {result_key}\")\n",
    "print(f\"  Attributes: {result['attributes']}\")\n",
    "print(f\"  Arrays available: {list(result['arrays'].keys())}\")\n",
    "\n",
    "# Access the pairs arrays (now split into indices and values)\n",
    "pairs_indices = result['arrays']['pairs_indices']\n",
    "pairs_values = result['arrays']['pairs_values']\n",
    "print(f\"  Pairs shape: {pairs_indices.shape}, values: {pairs_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb4d30",
   "metadata": {},
   "source": [
    "## Phase 4B: Auto-Save/Load with compare_datasets()\n",
    "\n",
    "The `compare_datasets()` function now supports automatic result caching via HDF5:\n",
    "- **save_path**: Path to HDF5 file for caching results\n",
    "- **regenerate**: Force recomputation even if cached result exists\n",
    "- **dataset_names**: Required for mode=\"between\" to identify datasets in cache\n",
    "\n",
    "Benefits:\n",
    "- **Instant loading**: Skip expensive computations for repeated analyses\n",
    "- **Reproducibility**: Cached results with metadata\n",
    "- **Easy comparison**: Test different metrics without recomputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e722659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUTO-SAVE/LOAD DEMONSTRATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import metrics functions\n",
    "from neural_analysis.metrics.pairwise_metrics import compare_datasets\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate test datasets\n",
    "np.random.seed(42)\n",
    "control_data = np.random.randn(100, 10)\n",
    "treatment_data = np.random.randn(100, 10) + 0.5  # Shifted distribution\n",
    "\n",
    "# Path for caching\n",
    "cache_file = base / \"comparison_cache.h5\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUTO-SAVE/LOAD DEMONSTRATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821e305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. First call: Computing and saving...\n",
      "   Result: 5.586092\n",
      "   Time: 1.9650s\n",
      "   Cache file created: True\n"
     ]
    }
   ],
   "source": [
    "# First call: Compute and save\n",
    "print(\"\\n1. First call: Computing and saving...\")\n",
    "start = time.time()\n",
    "result1 = compare_datasets(\n",
    "    control_data,\n",
    "    treatment_data,\n",
    "    mode=\"between\",\n",
    "    metric=\"wasserstein\",\n",
    "    save_path=cache_file,\n",
    "    dataset_names=(\"control\", \"treatment\"),\n",
    ")\n",
    "elapsed1 = time.time() - start\n",
    "\n",
    "# Result is a dict with 'value' key for between mode\n",
    "result1_value = result1['value'] if isinstance(result1, dict) else result1\n",
    "print(f\"   Result: {result1_value:.6f}\")\n",
    "print(f\"   Time: {elapsed1:.4f}s\")\n",
    "print(f\"   Cache file created: {cache_file.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8266e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Second call: Loading from cache...\n",
      "   Result: 5.586092\n",
      "   Time: 0.0018s\n",
      "   Speedup: 1110.7x faster!\n",
      "   Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Second call: Load from cache (instant!)\n",
    "print(\"\\n2. Second call: Loading from cache...\")\n",
    "start = time.time()\n",
    "result2 = compare_datasets(\n",
    "    control_data,  # These datasets are ignored - loads from cache\n",
    "    treatment_data,\n",
    "    mode=\"between\",\n",
    "    metric=\"wasserstein\",\n",
    "    save_path=cache_file,\n",
    "    dataset_names=(\"control\", \"treatment\"),\n",
    "    regenerate=False,  # Default: use cached result\n",
    ")\n",
    "elapsed2 = time.time() - start\n",
    "\n",
    "# When loading from cache, result is a float directly\n",
    "result2_value = result2 if isinstance(result2, (int, float)) else result2['value']\n",
    "print(f\"   Result: {result2_value:.6f}\")\n",
    "print(f\"   Time: {elapsed2:.4f}s\")\n",
    "print(f\"   Speedup: {elapsed1/elapsed2:.1f}x faster!\")\n",
    "print(f\"   Results match: {result2_value == result1_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88688bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Force regeneration (regenerate=True)...\n",
      "   Result: 10.520632\n",
      "   Time: 0.0012s\n",
      "   Changed: True\n"
     ]
    }
   ],
   "source": [
    "# Force regeneration with modified data\n",
    "print(\"\\n3. Force regeneration (regenerate=True)...\")\n",
    "treatment_modified = treatment_data + 0.5  # Further shift\n",
    "\n",
    "start = time.time()\n",
    "result3 = compare_datasets(\n",
    "    control_data,\n",
    "    treatment_modified,  # Different data\n",
    "    mode=\"between\",\n",
    "    metric=\"wasserstein\",\n",
    "    save_path=cache_file,\n",
    "    dataset_names=(\"control\", \"treatment\"),\n",
    "    regenerate=True,  # Force recomputation\n",
    ")\n",
    "elapsed3 = time.time() - start\n",
    "\n",
    "# Result is a dict with 'value' key for between mode\n",
    "result3_value = result3['value'] if isinstance(result3, dict) else result3\n",
    "print(f\"   Result: {result3_value:.6f}\")\n",
    "print(f\"   Time: {elapsed3:.4f}s\")\n",
    "print(f\"   Changed: {abs(result3_value - result1_value) > 0.001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03eaba",
   "metadata": {},
   "source": [
    "### All-Pairs Mode with Caching\n",
    "\n",
    "All-pairs mode also supports caching for efficiency when comparing multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e44ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mauls/Documents/Code/neural-analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to save result: cannot access local variable 'save_val_all_pairs' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ALL-PAIRS CACHING\n",
      "============================================================\n",
      "\n",
      "1. Computing all-pairs (6 comparisons)...\n",
      "   Time: 0.0497s\n",
      "   Comparisons: 16 pairs\n",
      "\n",
      "   Sample results:\n",
      "      control → control: 0.0000\n",
      "      control → treatment_A: 3.4233\n",
      "      control → treatment_B: 5.3701\n",
      "      control → treatment_C: 8.8483\n",
      "      treatment_A → control: 3.4233\n",
      "      treatment_A → treatment_A: 0.0000\n",
      "      treatment_A → treatment_B: 2.6983\n",
      "      treatment_A → treatment_C: 5.7887\n"
     ]
    }
   ],
   "source": [
    "# Create multiple datasets\n",
    "datasets = {\n",
    "    \"control\": np.random.randn(50, 8),\n",
    "    \"treatment_A\": np.random.randn(50, 8) + 0.3,\n",
    "    \"treatment_B\": np.random.randn(50, 8) + 0.7,\n",
    "    \"treatment_C\": np.random.randn(50, 8) + 1.0,\n",
    "}\n",
    "\n",
    "cache_all_pairs = base / \"all_pairs_cache.h5\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL-PAIRS CACHING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First call: Compute all pairs\n",
    "print(\"\\n1. Computing all-pairs (6 comparisons)...\")\n",
    "start = time.time()\n",
    "all_pairs_results = compare_datasets(\n",
    "    datasets,\n",
    "    mode=\"all-pairs\",\n",
    "    metric=\"wasserstein\",\n",
    "    save_path=cache_all_pairs,\n",
    "    show_progress=False,\n",
    ")\n",
    "elapsed_compute = time.time() - start\n",
    "\n",
    "print(f\"   Time: {elapsed_compute:.4f}s\")\n",
    "print(f\"   Comparisons: {sum(len(v) for v in all_pairs_results.values())} pairs\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n   Sample results:\")\n",
    "for i, (key_i, inner) in enumerate(all_pairs_results.items()):\n",
    "    if i < 2:  # Show first 2 datasets\n",
    "        for key_j, dist in inner.items():\n",
    "            print(f\"      {key_i} → {key_j}: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b68d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Loading from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing wasserstein distances: 100%|██████████| 16/16 [00:00<00:00, 7110.50it/s]\n",
      "Failed to save result: cannot access local variable 'save_val_all_pairs' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time: 0.0046s\n",
      "   Speedup: 10.9x faster!\n",
      "   Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Second call: Load from cache\n",
    "print(\"\\n2. Loading from cache...\")\n",
    "start = time.time()\n",
    "loaded_results = compare_datasets(\n",
    "    datasets,\n",
    "    mode=\"all-pairs\",\n",
    "    metric=\"wasserstein\",\n",
    "    save_path=cache_all_pairs,\n",
    "    regenerate=False,\n",
    ")\n",
    "elapsed_load = time.time() - start\n",
    "\n",
    "print(f\"   Time: {elapsed_load:.4f}s\")\n",
    "print(f\"   Speedup: {elapsed_compute/elapsed_load:.1f}x faster!\")\n",
    "\n",
    "# Verify results match\n",
    "matches = all(\n",
    "    loaded_results[k1][k2] == all_pairs_results[k1][k2]\n",
    "    for k1 in all_pairs_results\n",
    "    for k2 in all_pairs_results[k1]\n",
    ")\n",
    "print(f\"   Results match: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eefa7",
   "metadata": {},
   "source": [
    "### Inspection with comparison_store\n",
    "\n",
    "Use the comparison_store API to inspect cached comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3af3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSPECTING CACHED COMPARISONS\n",
      "============================================================\n",
      "\n",
      "Cached comparisons (between-mode):\n",
      "        metric mode dataset_i dataset_j\n",
      "0  wasserstein                         \n",
      "\n",
      "Direct load result: 10.520632\n",
      "Matches cached: False\n"
     ]
    }
   ],
   "source": [
    "# Query cached comparisons\n",
    "from neural_analysis.utils.comparison_store import query_comparisons, load_comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSPECTING CACHED COMPARISONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Query all comparisons in the between-mode cache\n",
    "df = query_comparisons(cache_file)\n",
    "print(\"\\nCached comparisons (between-mode):\")\n",
    "print(df[[\"metric\", \"mode\", \"dataset_i\", \"dataset_j\"]])\n",
    "\n",
    "# Load specific comparison directly\n",
    "direct_load = load_comparison(\n",
    "    cache_file,\n",
    "    metric=\"wasserstein\",\n",
    "    dataset_i=\"control\",\n",
    "    dataset_j=\"treatment\",\n",
    ")\n",
    "print(f\"\\nDirect load result: {direct_load:.6f}\")\n",
    "print(f\"Matches cached: {direct_load == result2_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33209655",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "\n",
    "**When to use auto-save:**\n",
    "- ✅ Expensive computations (high-D data, many samples)\n",
    "- ✅ Repeated analyses with same data\n",
    "- ✅ Batch processing pipelines\n",
    "- ✅ Exploratory data analysis workflows\n",
    "\n",
    "**When regenerate=True:**\n",
    "- ✅ Data has been updated\n",
    "- ✅ Metric parameters changed\n",
    "- ✅ Force cache refresh\n",
    "- ✅ Debugging/validation\n",
    "\n",
    "**Caching tips:**\n",
    "- Use descriptive dataset_names for easy identification\n",
    "- Organize cache files by experiment/session\n",
    "- Query comparisons to avoid redundant computation\n",
    "- Clean up old cache files periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "536247b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned up temporary directory\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "tmpdir.cleanup()\n",
    "print(\"✓ Cleaned up temporary directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-analysis",
   "language": "python",
   "name": "neural-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
